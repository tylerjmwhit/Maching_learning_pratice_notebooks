# Maching_learning_pratice_notebooks
These are various mahcine learnings tasks I did for ES 514 at Sonoma State. They showcase my work in python utilizing various machine learning and data science libraries

# Description of files
### Week 2 lab
- The code loads three datasets: Iris, MNIST, and Titanic, performing basic exploratory data analysis tasks such as counting target classifications, plotting data points, handling missing values, and creating new columns based on existing data. It demonstrates skills in data loading, manipulation, visualization, and preprocessing.

### Week 5 assingment
- Task 1 involves creating a 2D landscape plot of a mathematical function, performing gradient descent optimization, and visualizing the optimization process in both 3D and 2D plots.
- Task 2 involves implementing gradient descent for optimizing parameters of a function and testing it with the Iris dataset, varying the learning rate to observe its effect on convergence.

### Week 7 Lab
- The code showcases skills in data preprocessing, linear regression modeling, cross-validation using KFold, and feature engineering by incorporating time-related features from the date column for improved predictive performance.

### Week 8 Lab
- The code demonstrates model tuning using RandomizedSearchCV from scikit-learn, alongside model building with Keras Sequential API for a neural network classifier, incorporating dense layers and activation functions.
  
### Week 11 lab 
-This code showcases proficiency in data preprocessing, dimensionality reduction with PCA and Incremental PCA, model evaluation, and data visualization, particularly in comparing the impact of dimensionality reduction on model accuracy for both the Titanic dataset and count vectorized movie reviews.

### Week 13 assingment
-This code demonstrates proficiency in data preprocessing, clustering using KMeans, AgglomerativeClustering, and DBSCAN, model building with autoencoders, and image manipulation techniques like Gaussian blur. Additionally, it includes visualization skills to compare input and output images and assess model performance.

### Week 14 assingment
- The code trains two recurrent neural network (RNN) models, one using a GRU cell and the other using an LSTM cell, on the MNIST dataset to classify handwritten digits. Both models achieve good accuracy, demonstrating proficiency in implementing RNN architectures for image classification tasks. Additionally, the code showcases preprocessing steps like one-hot encoding and visualizing the dataset.












































