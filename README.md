# Maching_learning_pratice_notebooks
These are various mahcine learnings tasks I did for ES 514 at Sonoma State. They showcase my work in python utilizing various machine learning and data science libraries

# Description of files
### Week 1 Lab
- This code demonstrates proficiency in loading and visualizing the MNIST dataset using NumPy and Matplotlib, showcasing the average image of each digit. Additionally, it showcases the application of logistic regression for multi-class classification using the Iris dataset, evaluating precision, recall, and confusion matrix for each class.
- 
### Week 2 lab
- The code loads three datasets: Iris, MNIST, and Titanic, performing basic exploratory data analysis tasks such as counting target classifications, plotting data points, handling missing values, and creating new columns based on existing data. It demonstrates skills in data loading, manipulation, visualization, and preprocessing.

### Week 4 Lab
- This code utilizes k-fold cross-validation with logistic and linear regression models to evaluate performance on different datasets. It demonstrates the versatility of linear regression for numeric prediction tasks, such as predicting age in the Titanic dataset based on various input features.

### Week 5 Assingment
- Task 1 involves creating a 2D landscape plot of a mathematical function, performing gradient descent optimization, and visualizing the optimization process in both 3D and 2D plots.
- Task 2 involves implementing gradient descent for optimizing parameters of a function and testing it with the Iris dataset, varying the learning rate to observe its effect on convergence.

### Week 7 Lab
- The code showcases skills in data preprocessing, linear regression modeling, cross-validation using KFold, and feature engineering by incorporating time-related features from the date column for improved predictive performance.

### Week 8 Lab
- The code demonstrates model tuning using RandomizedSearchCV from scikit-learn, alongside model building with Keras Sequential API for a neural network classifier, incorporating dense layers and activation functions.

### Week 10 Assingment
- In Part 1, a Random Forest classifier was optimized using a grid search on the breast cancer dataset, yielding the best parameters with an accuracy of approximately 96.8%. In Part 2, individual classifiers (Random Forest, Decision Tree, and K-Nearest Neighbors) trained on the MNIST dataset achieved accuracies of around 96.9%, 87.7%, and 96.9% respectively. Combining their predictions with a hard voting classifier resulted in an accuracy of approximately 96.8%, outperforming any individual classifier.
  
### Week 11 lab 
-This code showcases proficiency in data preprocessing, dimensionality reduction with PCA and Incremental PCA, model evaluation, and data visualization, particularly in comparing the impact of dimensionality reduction on model accuracy for both the Titanic dataset and count vectorized movie reviews.

### Week 11 Assingment
-PCA was applied to facial keypoints images, retaining over 95% of the explained variance with 127 principal components. Evaluation showed that a feedforward neural network trained on the PCA-transformed dataset outperformed the model trained on the original dataset alone, as evidenced by a lower mean squared error.

### Week 13 Assingment
-This code demonstrates proficiency in data preprocessing, clustering using KMeans, AgglomerativeClustering, and DBSCAN, model building with autoencoders, and image manipulation techniques like Gaussian blur. Additionally, it includes visualization skills to compare input and output images and assess model performance.

### Week 14 Lab
-This code segment mounts Google Drive, preprocesses a text corpus from Shakespeare's sonnets, creates an RNN model using Keras to predict the next character from the previous 20 letters, trains the model on the data, and generates new text based on the trained model. The generated text appears to be nonsensical, suggesting that the model may require further tuning, such as increasing network depth or training for more epochs, to produce meaningful results.

### Week 14 Assingment
- The code trains two recurrent neural network (RNN) models, one using a GRU cell and the other using an LSTM cell, on the MNIST dataset to classify handwritten digits. Both models achieve good accuracy, demonstrating proficiency in implementing RNN architectures for image classification tasks. Additionally, the code showcases preprocessing steps like one-hot encoding and visualizing the dataset.












































