{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwflUJEVZeNtj4gOrIyiWn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tylerjmwhit/Maching_learning_pratice_notebooks/blob/main/week4_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9T2JOD_5xEa",
        "outputId": "37d05f09-9ab9-4d72-a82d-53ff440da20f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "(70000,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.datasets.mnist import load_data\n",
        "(train_x, train_y), (test_x, test_y) = load_data()\n",
        "\n",
        "com_x = np.append(train_x,test_x)\n",
        "com_x = com_x.reshape(70000,-1)\n",
        "com_y = np.append(train_y,test_y)\n",
        "print(com_y.shape)\n",
        "com_x.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logR = LogisticRegression()\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "scores = []\n",
        "for train_index, test_index in kf.split(com_x):\n",
        "  x_train, x_test = com_x[train_index] , com_x[test_index]\n",
        "  y_train, y_test = com_y[train_index] , com_y[test_index]\n",
        "  logR.fit(x_train,y_train)\n",
        "  scores.append(logR.score(x_test,y_test))\n",
        "print(scores)\n",
        "print(np.mean(scores))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vm69wcD6eMo",
        "outputId": "e79fe3d9-4f79-420d-d47b-1b24b7acbaba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9225, 0.9215, 0.9222857142857143, 0.9201428571428572, 0.9198571428571428]\n",
            "0.9212571428571428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJSK3yKkAfHZ",
        "outputId": "4cea50da-c060-4a42-a8e2-27cf3d98d782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic = pd.read_csv('/content/drive/My Drive/EE485_dataSets/titanic_train.csv')\n",
        "print(titanic)\n",
        "titanic = titanic.dropna(subset=['Age'])\n",
        "print(titanic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnizRLkZBrw3",
        "outputId": "e2679ca5-d00f-4808-fb71-7d9159ff17f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n",
            "0              1         0       3  ...   7.2500   NaN         S\n",
            "1              2         1       1  ...  71.2833   C85         C\n",
            "2              3         1       3  ...   7.9250   NaN         S\n",
            "3              4         1       1  ...  53.1000  C123         S\n",
            "4              5         0       3  ...   8.0500   NaN         S\n",
            "..           ...       ...     ...  ...      ...   ...       ...\n",
            "886          887         0       2  ...  13.0000   NaN         S\n",
            "887          888         1       1  ...  30.0000   B42         S\n",
            "888          889         0       3  ...  23.4500   NaN         S\n",
            "889          890         1       1  ...  30.0000  C148         C\n",
            "890          891         0       3  ...   7.7500   NaN         Q\n",
            "\n",
            "[891 rows x 12 columns]\n",
            "     PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n",
            "0              1         0       3  ...   7.2500   NaN         S\n",
            "1              2         1       1  ...  71.2833   C85         C\n",
            "2              3         1       3  ...   7.9250   NaN         S\n",
            "3              4         1       1  ...  53.1000  C123         S\n",
            "4              5         0       3  ...   8.0500   NaN         S\n",
            "..           ...       ...     ...  ...      ...   ...       ...\n",
            "885          886         0       3  ...  29.1250   NaN         Q\n",
            "886          887         0       2  ...  13.0000   NaN         S\n",
            "887          888         1       1  ...  30.0000   B42         S\n",
            "889          890         1       1  ...  30.0000  C148         C\n",
            "890          891         0       3  ...   7.7500   NaN         Q\n",
            "\n",
            "[714 rows x 12 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.api.types import is_numeric_dtype\n",
        "numeric_columns = [col for col in titanic.columns if is_numeric_dtype(titanic[col])]\n",
        "numeric_columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Mn1IJ3sEF5p",
        "outputId": "0c649180-7596-4c32-ba64-d73a9b623357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x = everything but the age\n",
        "# y = age\n",
        "\n",
        "numeric_columns= np.delete(numeric_columns,np.where(numeric_columns=='Age'))\n",
        "x = titanic[numeric_columns].values\n",
        "y = titanic['Age'].values\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "2tsoPcfbEHfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d554eec-abaa-486c-9059-fc96594df166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  1.       0.       3.     ...   1.       0.       7.25  ]\n",
            " [  2.       1.       1.     ...   1.       0.      71.2833]\n",
            " [  3.       1.       3.     ...   0.       0.       7.925 ]\n",
            " ...\n",
            " [888.       1.       1.     ...   0.       0.      30.    ]\n",
            " [890.       1.       1.     ...   0.       0.      30.    ]\n",
            " [891.       0.       3.     ...   0.       0.       7.75  ]]\n",
            "[22.   38.   26.   35.   35.   54.    2.   27.   14.    4.   58.   20.\n",
            " 39.   14.   55.    2.   31.   35.   34.   15.   28.    8.   38.   19.\n",
            " 40.   66.   28.   42.   21.   18.   14.   40.   27.    3.   19.   18.\n",
            "  7.   21.   49.   29.   65.   21.   28.5   5.   11.   22.   38.   45.\n",
            "  4.   29.   19.   17.   26.   32.   16.   21.   26.   32.   25.    0.83\n",
            " 30.   22.   29.   28.   17.   33.   16.   23.   24.   29.   20.   46.\n",
            " 26.   59.   71.   23.   34.   34.   28.   21.   33.   37.   28.   21.\n",
            " 38.   47.   14.5  22.   20.   17.   21.   70.5  29.   24.    2.   21.\n",
            " 32.5  32.5  54.   12.   24.   45.   33.   20.   47.   29.   25.   23.\n",
            " 19.   37.   16.   24.   22.   24.   19.   18.   19.   27.    9.   36.5\n",
            " 42.   51.   22.   55.5  40.5  51.   16.   30.   44.   40.   26.   17.\n",
            "  1.    9.   45.   28.   61.    4.    1.   21.   56.   18.   50.   30.\n",
            " 36.    9.    1.    4.   45.   40.   36.   32.   19.   19.    3.   44.\n",
            " 58.   42.   24.   28.   34.   45.5  18.    2.   32.   26.   16.   40.\n",
            " 24.   35.   22.   30.   31.   27.   42.   32.   30.   16.   27.   51.\n",
            " 38.   22.   19.   20.5  18.   35.   29.   59.    5.   24.   44.    8.\n",
            " 19.   33.   29.   22.   30.   44.   25.   24.   37.   54.   29.   62.\n",
            " 30.   41.   29.   30.   35.   50.    3.   52.   40.   36.   16.   25.\n",
            " 58.   35.   25.   41.   37.   63.   45.    7.   35.   65.   28.   16.\n",
            " 19.   33.   30.   22.   42.   22.   26.   19.   36.   24.   24.   23.5\n",
            "  2.   50.   19.    0.92 17.   30.   30.   24.   18.   26.   28.   43.\n",
            " 26.   24.   54.   31.   40.   22.   27.   30.   22.   36.   61.   36.\n",
            " 31.   16.   45.5  38.   16.   29.   41.   45.   45.    2.   24.   28.\n",
            " 25.   36.   24.   40.    3.   42.   23.   15.   25.   28.   22.   38.\n",
            " 40.   29.   45.   35.   30.   60.   24.   25.   18.   19.   22.    3.\n",
            " 22.   27.   20.   19.   42.    1.   32.   35.   18.    1.   36.   17.\n",
            " 36.   21.   28.   23.   24.   22.   31.   46.   23.   28.   39.   26.\n",
            " 21.   28.   20.   34.   51.    3.   21.   33.   44.   34.   18.   30.\n",
            " 10.   21.   29.   28.   18.   28.   19.   32.   28.   42.   17.   50.\n",
            " 14.   21.   24.   64.   31.   45.   20.   25.   28.    4.   13.   34.\n",
            "  5.   52.   36.   30.   49.   29.   65.   50.   48.   34.   47.   48.\n",
            " 38.   56.    0.75 38.   33.   23.   22.   34.   29.   22.    2.    9.\n",
            " 50.   63.   25.   35.   58.   30.    9.   21.   55.   71.   21.   54.\n",
            " 25.   24.   17.   21.   37.   16.   18.   33.   28.   26.   29.   36.\n",
            " 54.   24.   47.   34.   36.   32.   30.   22.   44.   40.5  50.   39.\n",
            " 23.    2.   17.   30.    7.   45.   30.   22.   36.    9.   11.   32.\n",
            " 50.   64.   19.   33.    8.   17.   27.   22.   22.   62.   48.   39.\n",
            " 36.   40.   28.   24.   19.   29.   32.   62.   53.   36.   16.   19.\n",
            " 34.   39.   32.   25.   39.   54.   36.   18.   47.   60.   22.   35.\n",
            " 52.   47.   37.   36.   49.   49.   24.   44.   35.   36.   30.   27.\n",
            " 22.   40.   39.   35.   24.   34.   26.    4.   26.   27.   42.   20.\n",
            " 21.   21.   61.   57.   21.   26.   80.   51.   32.    9.   28.   32.\n",
            " 31.   41.   20.   24.    2.    0.75 48.   19.   56.   23.   18.   21.\n",
            " 18.   24.   32.   23.   58.   50.   40.   47.   36.   20.   32.   25.\n",
            " 43.   40.   31.   70.   31.   18.   24.5  18.   43.   36.   27.   20.\n",
            " 14.   60.   25.   14.   19.   18.   15.   31.    4.   25.   60.   52.\n",
            " 44.   49.   42.   18.   35.   18.   25.   26.   39.   45.   42.   22.\n",
            " 24.   48.   29.   52.   19.   38.   27.   33.    6.   17.   34.   50.\n",
            " 27.   20.   30.   25.   25.   29.   11.   23.   23.   28.5  48.   35.\n",
            " 36.   21.   24.   31.   70.   16.   30.   19.   31.    4.    6.   33.\n",
            " 23.   48.    0.67 28.   18.   34.   33.   41.   20.   36.   16.   51.\n",
            " 30.5  32.   24.   48.   57.   54.   18.    5.   43.   13.   17.   29.\n",
            " 25.   25.   18.    8.    1.   46.   16.   25.   39.   49.   31.   30.\n",
            " 30.   34.   31.   11.    0.42 27.   31.   39.   18.   39.   33.   26.\n",
            " 39.   35.    6.   30.5  23.   31.   43.   10.   52.   27.   38.   27.\n",
            "  2.    1.   62.   15.    0.83 23.   18.   39.   21.   32.   20.   16.\n",
            " 30.   34.5  17.   42.   35.   28.    4.   74.    9.   16.   44.   18.\n",
            " 45.   51.   24.   41.   21.   48.   24.   42.   27.   31.    4.   26.\n",
            " 47.   33.   47.   28.   15.   20.   19.   56.   25.   33.   22.   28.\n",
            " 25.   39.   27.   19.   26.   32.  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def runlr(x,y):\n",
        "  from sklearn.linear_model import LinearRegression\n",
        "  lr = LinearRegression()\n",
        "  kf_titanic = KFold(n_splits=5, shuffle=True)\n",
        "  scores = []\n",
        "  for train_index, test_index in kf.split(x):\n",
        "    x_train, x_test = x[train_index] , x[test_index]\n",
        "    y_train, y_test = y[train_index] , y[test_index]\n",
        "    lr.fit(x_train,y_train)\n",
        "    scores.append(lr.score(x_test,y_test))\n",
        "  print(scores)\n",
        "  print(np.mean(scores))\n",
        "  return np.mean(scores)\n",
        "\n"
      ],
      "metadata": {
        "id": "AZ4X6lEkFCkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = titanic[['Survived', 'Pclass', 'Fare','SibSp']].values\n",
        "x2 = titanic[['Pclass', 'Parch', 'Fare', 'SibSp']].values\n",
        "\n",
        "scores1 = runlr(x,y)\n",
        "scores2 = runlr(x1,y)\n",
        "scores3 = runlr(x2,y)\n",
        "\n",
        "print(scores1,scores2,scores3)"
      ],
      "metadata": {
        "id": "FG-ZQKxqNSi5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}